{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ドイツ の 旅 ２ 「 世界 遺産 \\u3000 ハイデルベルク 城 」 \\n' 'タイ の 危険 地帯 \\n'\n",
      " '【 無計画 】 もう やん の 自転車 １ 週間 １ 人 旅 ～ ４ \\n' ...,\n",
      " '臨時 急行 き た ぐに \\u3000 京都 駅 発車 \\n' '京王 線 ・ 地上 へ ！ \\n'\n",
      " '錦江湾 サマー ナイト 大 花火 大会 2014 鹿児島 市 ダイジェスト \\n']\n",
      "['AW ニュース Weekly 『 昭和 天皇 実録 』 公表 \\u3000 100 \\n'\n",
      " '侍 ＪＰ さん と お話 です 。 vol 1 \\n'\n",
      " '【 政見 放送 】 2010 参院 選 東京 都 選挙 区 \\u3000 松本 みのる \\n' ...,\n",
      " '【 新 唐人 】 演壇 から 下ろさ れ た 有名 教授 と 裸 の 京劇 \\n'\n",
      " '日本 が 韓国 を 近代 国家 に し て しまっ た こと を 謝罪 しよ う パレード ⑤ \\n'\n",
      " '字幕 【 テキサス 親父 】 \\u3000 また も 笑わせる シー・シェパード ！ 危機 ？ \\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/yukihiro-su/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911122144985\n",
      "0.941410129096\n",
      "[[955  42]\n",
      " [ 76 941]]\n",
      "  (0, 13785)\t0.321620610544\n",
      "  (0, 11658)\t0.425596245202\n",
      "  (0, 5699)\t0.372989146524\n",
      "  (0, 5525)\t0.446077558363\n",
      "  (0, 4941)\t0.460609281312\n",
      "  (0, 3453)\t0.337976110415\n",
      "  (0, 3159)\t0.225683774785\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import types\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# NLP module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #名称変更\n",
    "\"線形SVMを利用するためにLinearSVMクラスをインポート\"\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\"学習データと評価データを分割するための関数をインポート\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import MeCab\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "output1 = []\n",
    "output2 = []\n",
    "\n",
    "f1 = open('/Users/yukihiro-su/Desktop/genso.csv') # 旅行のカテゴリのタイトル\n",
    "data1 = f1.read()  # ファイル終端まで全て読んだデータを返す\n",
    "f1.close()\n",
    "\n",
    "f2 = open('/Users/yukihiro-su/Desktop/mazime.csv') # 政治のカテゴリのタイトル\n",
    "data2 = f2.read()  # ファイル終端まで全て読んだデータを返す\n",
    "f2.close()\n",
    "\n",
    "lines1 = data1.split(',') \n",
    "lines2 = data2.split(',') \n",
    "\n",
    "for line in lines1:\n",
    "    #print (line)\n",
    "    line = re.sub('[!-/:-@[-`{-~]', \"\", line) #半角記号\n",
    "    line = re.sub('u\"[︰-＠]\"', \"\", line) #全角記号\n",
    "    line = re.sub('\\n',\"\",line)\n",
    "    line = tagger.parse(line)\n",
    "    output1.append(line)\n",
    "\n",
    "for line in lines2:\n",
    "    #print (line)\n",
    "    line = re.sub('[!-/:-@[-`{-~]', \"\", line) #半角記号\n",
    "    line = re.sub('u\"[︰-＠]\"', \"\", line) #全角記号\n",
    "    line = re.sub('u\"【】「」\"', \"\", line) #全角記号\n",
    "    line = re.sub('\\n',\"\",line)\n",
    "    line = tagger.parse(line)\n",
    "    output2.append(line)\n",
    "    \n",
    "\n",
    "\n",
    "a1= np.array(output1)\n",
    "print(a1)\n",
    "a1.reshape(a1.shape[0],1)\n",
    "a2= np.array(output2)\n",
    "print(a2)\n",
    "a2.reshape(a2.shape[0],1)\n",
    "a = np.r_[a1,a2]\n",
    "\n",
    "\n",
    "# CountVectorizer\n",
    "#count = CountVectorizer()\n",
    "#bag = count.fit_transform(a)\n",
    "#print(count.vocabulary_)\n",
    "#print(bag.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "X = tfidf.fit_transform(a)\n",
    "#np.set_printoptions(precsion=2) #新しいバージョンから使用しない\n",
    "#print(vecs.toarray())\n",
    "\n",
    "b1 = [1]*(a1.shape[0])\n",
    "b1 = np.array(b1)\n",
    "b1.reshape(a1.shape[0],1)\n",
    "b2 = [2]*(a2.shape[0])\n",
    "b2 = np.array(b2)\n",
    "b2.reshape(a2.shape[0],1)\n",
    "#result = np.dstack((a,b)) #リストで返したい時\n",
    "y = np.r_[b1,b2]\n",
    "\n",
    "#print(x.shape[0])\n",
    "#print(x.shape[1])\n",
    "#print(y.shape[0])\n",
    "\n",
    "\n",
    "\"特徴量と正解ラベルを学習データと評価データへ分割(一応、二割を評価用にする。)\"\n",
    "\"data_train,data_test,label_train,label_test = train_test_split(data,mnist.target,test_size=0.2)\"\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n",
    "\n",
    "\"チューニングを行なって各種パラメータを調整する\"\n",
    "parameters = [\n",
    "       {'C': [1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "clf_svm = GridSearchCV(SVC(), parameters, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "lr2 = GridSearchCV(LogisticRegression(penalty='l2',multi_class='multinomial',solver='saga'), param_grid=parameters, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "lr2.fit(X_train, y_train)\n",
    "result_svm = clf_svm.predict(X_test)\n",
    "result_lr2 = lr2.predict(X_test)\n",
    "\n",
    "\"accuracyを求めて、格納する。\"\n",
    "accuracy1 = accuracy_score(y_test, result_svm)\n",
    "accuracy2 = accuracy_score(y_test, result_lr2)\n",
    "print(accuracy1)\n",
    "print(accuracy2)\n",
    "cmat = confusion_matrix(y_test, result_lr2)\n",
    "print(cmat)\n",
    "\n",
    "\n",
    "output = []\n",
    "# CountVectorizer\n",
    "count = CountVectorizer()\n",
    "\n",
    "#str = \"毎日残業疲れたから、もう幻想郷に旅行に行きたい\"\n",
    "result = tagger.parse(\"毎日サービス残業疲れたから、ハワイ、ドバイに行きたい\")\n",
    "output.append(result)\n",
    "fin = tfidf.transform(output)\n",
    "print(fin)\n",
    "print(lr2.predict(fin))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
